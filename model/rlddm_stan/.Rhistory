sub <- which(raw_data$subjID==subj)
raw_data[sub,]$trial <- seq.int(nrow(raw_data[sub,]))
}
# first is Sx1 matrix identifying all first trials of a subject for each choice
first <- which(raw_data$trial==1)
# if N=1 transform int to 1-d array
ifelse(is.null(dim(first)),first<-as.array(first))
# last is a Sx1 matrix identifying all last trials of a subject for each choice
last <- as.integer(first + DT_trials$N - 1)
ifelse(is.null(dim(last)),last<-as.array(last))
# incorrect is the inverse vector of choice and is needed to update the ev for the non-choices
raw_data$incorrect <- as.integer(ifelse(raw_data$correct==1, 0, 1))
# define the values for the rewards
value <- ifelse(raw_data$correct==1, 1, 0)
## all RT with negative choices -> -1
#new_RT <- ifelse(raw_data$correct==1, raw_data$RT*-1, raw_data$RT)
## # obs
n_trials <- nrow(raw_data)
##
stims <- raw_data$aStim
# encoding for simulation: lower (incorrect) response=1, upper (correct) response =2
raw_data$response = raw_data$correct+1
raw_data$nonresponse = abs(raw_data$correct-2)
dat <- list("N" = n_subj, "T"=n_trials,"RTbound" = 0.15,"minRT" = minRT, "iter" = raw_data$trial, "response" = raw_data$response,"nonresponse" = raw_data$nonresponse,
"RT" = raw_data$RT, "first" = first, "last" = last, "value"=value, "stims" = stims)  # names list of numbers
stanmodel_arg <- rstan::stan_model(model_path)
fit <- rstan::sampling(object  = stanmodel_arg,
data    = dat,
init    = "random",
chains  = 2,
iter    = 4000,
warmup  = 1000,
thin    = 1,
control = list(adapt_delta   = 0.95,
stepsize      = 1,
max_treedepth = 10),
verbose =TRUE)
.libPaths()
assign(".lib.loc", "C:/Program Files/R/R-3.5.2/library", envir = environment(.libPaths))
#install.packages("rstan", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("StanHeaders", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("rstantools", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("hBayesDM", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("Rcpp", lib="C:\\Program Files\\R\\R-3.5.2\\library")
# after successful installation, load required packages
library("StanHeaders", lib.loc="C:/Program Files/R/R-3.5.2/library")
library("rstan", lib.loc="C:/Program Files/R/R-3.5.2/library")
library("Rcpp", lib.loc="C:/Program Files/R/R-3.5.2/library")
library("hBayesDM", lib.loc="C:/Program Files/R/R-3.5.2/library")
### data loading and preprocessing
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
model_path <- paste0(path,"/rlddm.stan")
data_path <- paste0(path,"/test_input.txt")
setwd(path)
raw_data <- data.table::fread(file = data_path, header = TRUE, sep = "\t", data.table = TRUE,
fill = TRUE, stringsAsFactors = TRUE, logical01 = FALSE)
raw_data <- cbind(rep(substr(data_path,1,12),dim(raw_data)[1]),raw_data)
#D<-D[D$resp!=0,] # remove 'too slow ' responses
### Rename and transform some columns
colnames(raw_data)[1] <- "subjID"
raw_data$rt <- raw_data$rt/1000
names(raw_data)[names(raw_data)=="rt"] <- "RT"
# automatically filter missed responses (since RT = 0)
raw_data <- raw_data[which(raw_data$RT > 0.15),]
raw_data$subjID = rep('01',nrow(raw_data))
# fb = 0 incorrect, fb = 1 correct, (fb = 2 missed)
names(raw_data)[names(raw_data)=="fb"] <- "correct"
## prepare data for jags
#raw_data$row <- seq.int(nrow(raw_data))
DT_trials <- raw_data[, .N, by = "subjID"]
subjs     <- DT_trials$subjID
n_subj    <- length(subjs)
# get minRT
minRT <- with(raw_data, aggregate(RT, by = list(y = subjID), FUN = min)[["x"]])
ifelse(is.null(dim(minRT)),minRT<-as.array(minRT))
# assign new trial number for excluded decisions
for (subj in subjs){
sub <- which(raw_data$subjID==subj)
raw_data[sub,]$trial <- seq.int(nrow(raw_data[sub,]))
}
# first is Sx1 matrix identifying all first trials of a subject for each choice
first <- which(raw_data$trial==1)
# if N=1 transform int to 1-d array
ifelse(is.null(dim(first)),first<-as.array(first))
# last is a Sx1 matrix identifying all last trials of a subject for each choice
last <- as.integer(first + DT_trials$N - 1)
ifelse(is.null(dim(last)),last<-as.array(last))
# incorrect is the inverse vector of choice and is needed to update the ev for the non-choices
raw_data$incorrect <- as.integer(ifelse(raw_data$correct==1, 0, 1))
# define the values for the rewards
value <- ifelse(raw_data$correct==1, 1, 0)
## all RT with negative choices -> -1
#new_RT <- ifelse(raw_data$correct==1, raw_data$RT*-1, raw_data$RT)
## # obs
n_trials <- nrow(raw_data)
##
stims <- raw_data$aStim
# encoding for simulation: lower (incorrect) response=1, upper (correct) response =2
raw_data$response = raw_data$correct+1
raw_data$nonresponse = abs(raw_data$correct-2)
dat <- list("N" = n_subj, "T"=n_trials,"RTbound" = 0.15,"minRT" = minRT, "iter" = raw_data$trial, "response" = raw_data$response,"nonresponse" = raw_data$nonresponse,
"RT" = raw_data$RT, "first" = first, "last" = last, "value"=value, "stims" = stims)  # names list of numbers
View(dat)
fit <- rstan::sampling(object  = stanmodel_arg,
data    = dat,
init    = "random",
chains  = 2,
iter    = 4000,
warmup  = 1000,
thin    = 1,
control = list(adapt_delta   = 0.95,
stepsize      = 1,
max_treedepth = 10),
verbose =TRUE)
stanmodel_arg <- rstan::stan_model(model_path)
stanmodel_arg <- rstan::stan_model(model_path)
fit <- rstan::sampling(object  = stanmodel_arg,
data    = dat,
init    = "random",
chains  = 2,
iter    = 4000,
warmup  = 1000,
thin    = 1,
control = list(adapt_delta   = 0.95,
stepsize      = 1,
max_treedepth = 10),
verbose =TRUE)
parVals <- rstan::extract(fit, permuted = TRUE)
mean(parVals$eta_neg)
# eta is 3 dimensional: 1-d: iteration, 2-d: subject, 3-d: pos or neg eta
mean(parVals$eta_pos)
mean(parVals$tau)
mean(parVals$alpha)
print(fit_summary$summary)
fit_summary <- summary(fit)
# In fit_summary$summary all chains are merged whereas fit_summary$c_summary contains summaries for each chain individually.
# Typically we want the summary for all chains merged
print(names(fit_summary))
print(fit_summary$summary)
.libPaths()
assign(".lib.loc", "C:/Program Files/R/R-3.5.2/library", envir = environment(.libPaths))
#install.packages("rstan", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("StanHeaders", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("rstantools", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("hBayesDM", lib="C:\\Program Files\\R\\R-3.5.2\\library")
#install.packages("Rcpp", lib="C:\\Program Files\\R\\R-3.5.2\\library")
# after successful installation, load required packages
library("StanHeaders", lib.loc="C:/Program Files/R/R-3.5.2/library")
library("rstan", lib.loc="C:/Program Files/R/R-3.5.2/library")
library("Rcpp", lib.loc="C:/Program Files/R/R-3.5.2/library")
library("hBayesDM", lib.loc="C:/Program Files/R/R-3.5.2/library")
### data loading and preprocessing
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
model_path <- paste0(path,"/rlddm.stan")
data_path <- paste0(path,"/test_input.txt")
setwd(path)
raw_data <- data.table::fread(file = data_path, header = TRUE, sep = "\t", data.table = TRUE,
fill = TRUE, stringsAsFactors = TRUE, logical01 = FALSE)
raw_data <- cbind(rep(substr(data_path,1,12),dim(raw_data)[1]),raw_data)
#D<-D[D$resp!=0,] # remove 'too slow ' responses
### Rename and transform some columns
colnames(raw_data)[1] <- "subjID"
raw_data$rt <- raw_data$rt/1000
names(raw_data)[names(raw_data)=="rt"] <- "RT"
# automatically filter missed responses (since RT = 0)
raw_data <- raw_data[which(raw_data$RT > 0.15),]
raw_data$subjID = rep('01',nrow(raw_data))
# fb = 0 incorrect, fb = 1 correct, (fb = 2 missed)
names(raw_data)[names(raw_data)=="fb"] <- "correct"
## prepare data for jags
#raw_data$row <- seq.int(nrow(raw_data))
DT_trials <- raw_data[, .N, by = "subjID"]
subjs     <- DT_trials$subjID
n_subj    <- length(subjs)
# get minRT
minRT <- with(raw_data, aggregate(RT, by = list(y = subjID), FUN = min)[["x"]])
ifelse(is.null(dim(minRT)),minRT<-as.array(minRT))
# assign new trial number for excluded decisions
for (subj in subjs){
sub <- which(raw_data$subjID==subj)
raw_data[sub,]$trial <- seq.int(nrow(raw_data[sub,]))
}
# first is Sx1 matrix identifying all first trials of a subject for each choice
first <- which(raw_data$trial==1)
# if N=1 transform int to 1-d array
ifelse(is.null(dim(first)),first<-as.array(first))
# last is a Sx1 matrix identifying all last trials of a subject for each choice
last <- as.integer(first + DT_trials$N - 1)
ifelse(is.null(dim(last)),last<-as.array(last))
# incorrect is the inverse vector of choice and is needed to update the ev for the non-choices
raw_data$incorrect <- as.integer(ifelse(raw_data$correct==1, 0, 1))
# define the values for the rewards
value <- ifelse(raw_data$correct==1, 1, 0)
## all RT with negative choices -> -1
#new_RT <- ifelse(raw_data$correct==1, raw_data$RT*-1, raw_data$RT)
## # obs
n_trials <- nrow(raw_data)
##
stims <- raw_data$aStim
# encoding for simulation: lower (incorrect) response=1, upper (correct) response =2
raw_data$response = raw_data$correct+1
raw_data$nonresponse = abs(raw_data$correct-2)
dat <- list("N" = n_subj, "T"=n_trials,"RTbound" = 0.15,"minRT" = minRT, "iter" = raw_data$trial, "response" = raw_data$response,"nonresponse" = raw_data$nonresponse,
"RT" = raw_data$RT, "first" = first, "last" = last, "value"=value, "stims" = stims)  # names list of numbers
stanmodel_arg <- rstan::stan_model(model_path)
fit <- rstan::sampling(object  = stanmodel_arg,
data    = dat,
init    = "random",
chains  = 2,
iter    = 4000,
warmup  = 1000,
thin    = 1,
control = list(adapt_delta   = 0.95,
stepsize      = 1,
max_treedepth = 10),
verbose =TRUE)
parVals <- rstan::extract(fit, permuted = TRUE)
## now access the data of the fit
print(names(parVals))
fit_summary <- summary(fit)
print(fit_summary$summary)
posterior_means <- fit_summary$summary[,1]
posterior_means
posterior_means <- fit_summary$summary[,19:26]
posterior_means <- fit_summary$summary[19:26,1]
posterior_means
posterior_means <- fit_summary$summary[19:24,1]
posterior_means
posterior_means
posterior_means[,1]
typeOf(posterior_means)
typeof(posterior_means)
c(alpha,eta_pos,eta_neg,a_mod,v_mod,tau) <- posterior_means
install.packages("RWiener")
posterior_means(1)
posterior_means[1]
dims(posterior_means)
dim(posterior_means)
v_mod <- 0.5182816
eta_pos <- 0.6148918
eta_neg <- 0.7110666
alpha <- 1.5797116
a_mod <- 0.0769706
tau <- 0.6743383
ev <- matrix(data=0, nrow=dat$T,ncol=2)
pe <- list()
pe <- rep(0,dat$T)
delta <- list()
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:last[s]-1)){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + eta_neg * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + eta_pos * (dat$value[trial]-ev[trial,2])
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning <- cbind(ev,pe,dat$response,delta)
dat$response
first[s]
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:last[s]-1)){
print(trial)
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + eta_neg * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + eta_pos * (dat$value[trial]-ev[trial,2])
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
first[1]
dat$response[0]
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:last[s]-1)){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]+1==1){
ev[trial+1,1] <- ev[trial,1] + eta_neg * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + eta_pos * (dat$value[trial]-ev[trial,2])
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:last[s]-1)){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial+1]==1){
ev[trial+1,1] <- ev[trial,1] + eta_neg * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + eta_pos * (dat$value[trial]-ev[trial,2])
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
rm trial
remove(trial)
for(trial in (first[s]:last[s]-1)){
print(trial)
}
last[s]
first[s]
first[s]:last[s]
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:(last[s]-1))){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + eta_neg * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + eta_pos * (dat$value[trial]-ev[trial,2])
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning
value
v_mod <- 0.5182816
eta_pos <- 0.06148918
eta_neg <- 0.07110666
alpha <- 1.5797116
a_mod <- 0.0769706
tau <- 0.6743383
ev <- matrix(data=0, nrow=dat$T,ncol=2)
pe <- list()
pe <- rep(0,dat$T)
delta <- list()
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:(last[s]-1))){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + (eta_neg * (dat$value[trial]-ev[trial,1]))
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + (eta_pos * (dat$value[trial]-ev[trial,2]))
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning <- cbind(ev,dat$response,delta)
learning
inv.logit(0.5)
inv_logit(0.5)
library(boot) #needed for inverse logit
library(boot) #needed for inverse logit
inv_logit(0.5)
inv_logit
inv.logit()
inv.logit(0.5)
inv.logit(0.1)
logit(0.5)
logit(0.2)
inv.logit(-2)
stanmodel_invlog <- rstan::stan_model(model_path)
fit_invlog <- rstan::sampling(object  = stanmodel_invlog,
data    = dat,
init    = "random",
chains  = 2,
iter    = 4000,
warmup  = 1000,
thin    = 1,
control = list(adapt_delta   = 0.95,
stepsize      = 1,
max_treedepth = 10),
verbose =TRUE)
parValsinvl <- rstan::extract(fit_invlog, permuted = TRUE)
fit_summary_invlog <- summary(fit_invlog)
print(fit_summary_invlog$summary)
ev <- matrix(data=0, nrow=dat$T,ncol=2)
pe <- list()
pe <- rep(0,dat$T)
delta <- list()
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:(last[s]-1))){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + inv.logit(eta_neg * (dat$value[trial]-ev[trial,1]))
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + inv.logit(eta_pos * (dat$value[trial]-ev[trial,2]))
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning <- cbind(ev,dat$response,delta)
learning
v_mod <- 0.5182816
eta_pos <- 0.6148918
eta_neg <- 0.7110666
alpha <- 1.5797116
a_mod <- 0.0769706
tau <- 0.6743383
ev <- matrix(data=0, nrow=dat$T,ncol=2)
pe <- list()
pe <- rep(0,dat$T)
delta <- list()
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:(last[s]-1))){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + inv.logit(eta_neg * (dat$value[trial]-ev[trial,1]))
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + inv.logit(eta_pos * (dat$value[trial]-ev[trial,2]))
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning <- cbind(ev,dat$response,delta)
learning
ev <- matrix(data=0, nrow=dat$T,ncol=2)
pe <- list()
pe <- rep(0,dat$T)
delta <- list()
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:(last[s]-1))){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + inv.logit(eta_neg) * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + inv.logit(eta_pos) * (dat$value[trial]-ev[trial,2]))
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning <- cbind(ev,dat$response,delta)
ev <- matrix(data=0, nrow=dat$T,ncol=2)
pe <- list()
pe <- rep(0,dat$T)
delta <- list()
delta <- rep(0,dat$T)
for(s in 1:dat$N){
# initialize lower(=1) and upper(=2) bound value
ev[first[s],1] <- 0.5
ev[first[s],2] <- 0.5
for(trial in (first[s]:(last[s]-1))){
delta[trial] <- (ev[trial,2]-ev[trial,1]) * v_mod
if(dat$response[trial]==1){
ev[trial+1,1] <- ev[trial,1] + inv.logit(eta_neg) * (dat$value[trial]-ev[trial,1])
ev[trial+1,2] <- ev[trial,2]
}
else{
ev[trial+1,2] <- ev[trial,2] + inv.logit(eta_pos) * (dat$value[trial]-ev[trial,2])
ev[trial+1,1] <- ev[trial,1]
}
delta[last[s]] =  (ev[last[s]-1,2]-ev[last[s]-1,1]) * v_mod
}
}
learning <- cbind(ev,dat$response,delta)
learning
install.packages(c("dplyr", "readr"))
install.packages(c("dplyr", "readr"))
install.packages(c("dplyr", "readr"))
